## Gradient Method

```python
import numpy as np

# f = x^2 - x + 2
# df = 2x - 1

x = np.linspace(-1,3,20)
y = x**2 - x + 2

# Initial value 
x_init = 0

# Learning rate
alpha = 0.02   # 學習率太低，會造成需要迭代更多次，所以盡量高一些


x_old = 0; x_new = 0
for i in range(1, 1000):    # 迭代1000次
    x_old = x_new 
    x_new = x_old - alpha * ( 2*x_old -1 )
 
print(x_new)
```


## Golden section Method

```python
import math

def f(x):
    return x**2 - x + 2

def golden_section_method(a, b, epsilon):
    gr = (math.sqrt(5) + 1) / 2  # Golden ratio

    while abs(b - a) %3E epsilon:
        x1 = b - (b - a) / gr
        x2 = a + (b - a) / gr

        if f(x1) %3C f(x2):
            b = x2
        else:
            a = x1

    return (a + b) / 2

a = -1  # Initial interval start
b = 3   # Initial interval end
epsilon = 0.0001  # Desired precision

local_minimum = golden_section_method(a, b, epsilon)
print("The local minimum is approximately:", local_minimum)>)```

